import os
import argparse
import sys
from typing import Optional
from mistralai import Mistral
from mistralai.models import UserMessage, SystemMessage, AssistantMessage
import contextlib

@contextlib.contextmanager
def interactive_stdin_from_tty(is_piped: bool, should_redirect_tty: bool):
    """
    Context manager to redirect sys.stdin to /dev/tty if needed for interactive input
    after piped input. Ensures sys.stdin is restored afterwards.
    """
    original_stdin = sys.stdin
    tty_stdin = None
    
    # åªæœ‰åœ¨æ˜¯ç®¡é“è¾“å…¥ä¸”éœ€è¦é‡å®šå‘åˆ° TTY æ—¶æ‰æ‰§è¡Œ
    if is_piped and should_redirect_tty:
        try:
            # å°è¯•æ‰“å¼€ /dev/tty è¿›è¡Œäº¤äº’å¼è¾“å…¥
            # æ³¨æ„: è¿™æ˜¯ Unix-like ç³»ç»Ÿç‰¹æœ‰çš„ã€‚åœ¨ Windows ä¸Šéœ€è¦ä¸åŒçš„æ–¹æ³•ã€‚
            tty_stdin = open('/dev/tty', 'r')
            sys.stdin = tty_stdin
            print("å·²ä»ç®¡é“è¯»å–å†…å®¹ï¼Œç°åœ¨å°†è¿›å…¥äº¤äº’æ¨¡å¼ã€‚")
        except OSError:
            print("è­¦å‘Š: æ— æ³•æ‰“å¼€ /dev/tty è¿›è¡Œäº¤äº’å¼è¾“å…¥ã€‚å°†é€€å‡ºã€‚")
            sys.stdin = original_stdin # æ¢å¤åŸå§‹ stdin ä»¥é˜²ä¸‡ä¸€
            raise # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼ŒæŒ‡ç¤ºæ— æ³•è¿›å…¥äº¤äº’æ¨¡å¼
        except Exception as e:
            print(f"è­¦å‘Š: å°è¯•åˆ‡æ¢åˆ° /dev/tty æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}ã€‚å°†é€€å‡ºã€‚")
            sys.stdin = original_stdin # æ¢å¤åŸå§‹ stdin
            raise
    try:
        yield # æ‰§è¡Œ with å—å†…çš„ä»£ç 
    finally:
        # ç¡®ä¿åœ¨é€€å‡ºä¸Šä¸‹æ–‡æ—¶å…³é—­æ‰“å¼€çš„æ–‡ä»¶å¹¶æ¢å¤ sys.stdin
        if tty_stdin:
            tty_stdin.close()
        sys.stdin = original_stdin


class CLIAssistant:
    MODELS = {
        "command": "codestral-latest",
        "code": "codestral-latest",
        "general": "mistral-small-latest"
    }
    SYSTEM_PROMPTS = {
        "command": (
            "You are an expert macOS command-line assistant. Your task is to generate accurate, safe, and efficient shell commands. "
            "For each command, provide a brief explanation of its purpose and usage, including any important flags or considerations. "
            "Always present the command in a code block. Answer in English, followed by a Chinese explanation separated by '---'."
        ),
        "code": (
            "You are a highly skilled software engineer. Your responsibilities include breaking down complex requirements, "
            "designing robust architectures, writing clean, production-ready, and well-tested code, optimizing performance, "
            "and debugging issues. Prioritize code quality, readability, and adherence to best practices. "
            "Always present code snippets in appropriate code blocks. Answer in English, followed by a Chinese explanation separated by '---'."
        ),
        "general": (
            "You are a versatile and helpful AI assistant. Your capabilities include answering a wide range of questions, "
            "analyzing documents, interpreting images (if applicable), summarizing information, and solving complex problems across various domains. "
            "Strive for accuracy, clarity, and helpfulness in your responses. Answer in English, followed by a Chinese explanation separated by '---'."
        )
    }
    # æ·»åŠ æ¨¡å¼æ˜ å°„
    MODE_MAPPING = {
        "1": "command",
        "2": "code",
        "3": "general"
    }

    def __init__(self):
        self.api_key = os.getenv("MISTRAL_API_KEY")
        if not self.api_key:
            raise RuntimeError("MISTRAL_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")
        self.history = []

    async def generate(self, prompt, mode="command"):
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            SystemMessage(content=self.SYSTEM_PROMPTS[mode]),  # ç³»ç»Ÿæç¤º
            *self.history,  # ç›´æ¥æ‰©å±•å†å²è®°å½•
            UserMessage(content=prompt)  # å½“å‰ç”¨æˆ·è¾“å…¥
        ]

        async with Mistral(api_key=self.api_key) as client:
            response = await client.chat.stream_async(
                model=self.MODELS[mode],
                messages=messages
            )
            async for chunk in response:
                if content := chunk.data.choices[0].delta.content:
                    yield content

    # ä¿®æ”¹ run æ–¹æ³•çš„ç­¾åï¼Œæ·»åŠ  initial_prompt_arg å‚æ•°
    async def run(self, mode: str, initial_prompt_arg: Optional[str] = None):
        internal_mode = self.MODE_MAPPING.get(mode, mode)

        is_piped_input = not sys.stdin.isatty() # æ£€æŸ¥æ˜¯å¦æ˜¯ç®¡é“è¾“å…¥
        piped_content = None

        if is_piped_input:
            piped_content = sys.stdin.read().strip()
            if not piped_content:
                print("ç®¡é“è¾“å…¥ä¸ºç©ºã€‚")
                piped_content = None

        first_ai_prompt = None
        user_message_for_history = None
        
        # ç¡®å®šå‘é€ç»™ AI çš„ç¬¬ä¸€ä¸ªæç¤ºï¼Œä»¥åŠç”¨äºå†å²è®°å½•çš„ç”¨æˆ·æ¶ˆæ¯
        if initial_prompt_arg:
            if piped_content:
                first_ai_prompt = f"ä»¥ä¸‹æ˜¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n```\n{piped_content}\n```\n\nè¯·æ ¹æ®ä¸Šè¿°ä¸Šä¸‹æ–‡ï¼Œå›ç­”æˆ‘çš„é—®é¢˜ï¼š{initial_prompt_arg}"
                user_message_for_history = f"ä¸Šä¸‹æ–‡:\n```\n{piped_content}\n```\n\né—®é¢˜: {initial_prompt_arg}"
            else:
                first_ai_prompt = initial_prompt_arg
                user_message_for_history = initial_prompt_arg
        elif piped_content:
            first_ai_prompt = piped_content
            user_message_for_history = piped_content

        # å¤„ç†åˆå§‹è¯·æ±‚ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if first_ai_prompt:
            print(f"å¤„ç†åˆå§‹è¯·æ±‚ ({internal_mode} æ¨¡å¼)...")
            
            full_initial_response = "" # To collect response for history
            print(f"ğŸ§  å›ç­”:\n", end="", flush=True) # Print header once
            async for chunk in self.generate(first_ai_prompt, internal_mode):
                print(chunk, end="", flush=True)
                full_initial_response += chunk
            print() # Ensure a newline after the streamed output

            # å°†åˆå§‹äº¤äº’æ·»åŠ åˆ°å†å²è®°å½•ä¸­
            self.history.extend([
                UserMessage(content=user_message_for_history),
                AssistantMessage(content=full_initial_response.strip()) # Use collected full_initial_response
            ])
        
        # å†³å®šæ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼
        # åªæœ‰å½“ sys.stdin æ˜¯ä¸€ä¸ª TTY (å³éç®¡é“/é‡å®šå‘)
        # æˆ–è€…å½“æœ‰ç®¡é“è¾“å…¥ä½†åŒæ—¶æä¾›äº†å‘½ä»¤è¡Œåˆå§‹é—®é¢˜æ—¶ï¼Œæ‰è¿›å…¥äº¤äº’æ¨¡å¼ã€‚
        # å¦‚æœåªæœ‰ç®¡é“è¾“å…¥è€Œæ²¡æœ‰å‘½ä»¤è¡Œåˆå§‹é—®é¢˜ï¼Œåˆ™ä¸è¿›å…¥äº¤äº’æ¨¡å¼ã€‚
        should_enter_interactive_mode = (not is_piped_input) or \
                                        (is_piped_input and initial_prompt_arg is not None)

        if not should_enter_interactive_mode:
            return # å¦‚æœä¸è¿›å…¥äº¤äº’æ¨¡å¼ï¼Œåˆ™ç›´æ¥é€€å‡º

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¤„ç† sys.stdin çš„é‡å®šå‘å’Œæ¢å¤
        try:
            # should_redirect_tty å‚æ•°å†³å®šæ˜¯å¦çœŸçš„å°è¯•æ‰“å¼€ /dev/tty
            # åªæœ‰å½“æ˜¯ç®¡é“è¾“å…¥ä¸”éœ€è¦è¿›å…¥äº¤äº’æ¨¡å¼æ—¶æ‰é‡å®šå‘
            with interactive_stdin_from_tty(is_piped_input, is_piped_input and initial_prompt_arg is not None):
                print(f"æ¬¢è¿ä½¿ç”¨ CLI Assistant ({internal_mode} æ¨¡å¼) - è¾“å…¥ 'exit' é€€å‡º")
                while True:
                    try:
                        # input() å‡½æ•°ç°åœ¨ä¼šä½¿ç”¨å½“å‰ï¼ˆå¯èƒ½å·²é‡å®šå‘çš„ï¼‰sys.stdin
                        user_input = input("ğŸ‘‰ ").strip()
                    except EOFError:
                        print("\næ£€æµ‹åˆ° EOFï¼Œé€€å‡ºäº¤äº’æ¨¡å¼ã€‚")
                        break
                    
                    if user_input in ['exit', 'quit', 'e', 'q']:
                        print("å†è§ï¼")
                        break
                    if not user_input:
                        continue

                    full_interactive_response = "" # To collect response for history
                    print(f"ğŸ§  å›ç­”:\n", end="", flush=True) # Print header once
                    async for chunk in self.generate(user_input, internal_mode):
                        print(chunk, end="", flush=True)
                        full_interactive_response += chunk
                    print() # Ensure a newline after the streamed output

                    self.history.extend([
                        UserMessage(content=user_input),
                        AssistantMessage(content=full_interactive_response.strip()) # Use collected full_interactive_response
                    ])
        except Exception as e:
            # æ•è·æ¥è‡ª interactive_stdin_from_tty ä¸Šä¸‹ç®¡ç†å™¨çš„å¼‚å¸¸
            print(f"é”™è¯¯: æ— æ³•è¿›å…¥äº¤äº’æ¨¡å¼ã€‚{e}")
            return

if __name__ == "__main__":
    import asyncio

    parser = argparse.ArgumentParser(description="CLI Assistant powered by Mistral AI.")
    parser.add_argument(
        "-m", "--mode",
        choices=["1", "2", "3"],
        default="3",
        help="é€‰æ‹©AIæ¨¡å¼: '1' (å‘½ä»¤è¡Œå¤§å¸ˆ), '2' (è½¯ä»¶å·¥ç¨‹å¸ˆ), '3' (è¶…çº§åŠ©ç†). é»˜è®¤ä¸º '3'."
    )
    # æ·»åŠ ä¸€ä¸ªå¯é€‰çš„ positional argument ç”¨äºåˆå§‹æç¤º
    parser.add_argument(
        "initial_prompt",
        nargs="?", # ä½¿å…¶æˆä¸ºå¯é€‰å‚æ•° (0 æˆ– 1 ä¸ª)
        help="å¯é€‰çš„åˆå§‹æç¤ºï¼Œç”¨äºåœ¨ç®¡é“è¾“å…¥åç«‹å³æé—®ï¼Œæˆ–ä½œä¸ºç¬¬ä¸€ä¸ªé—®é¢˜ã€‚"
    )
    args = parser.parse_args()

    # å°†æ–°çš„ initial_prompt å‚æ•°ä¼ é€’ç»™ run æ–¹æ³•
    asyncio.run(CLIAssistant().run(args.mode, args.initial_prompt))
